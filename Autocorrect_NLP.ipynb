{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autocorrect\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='0'></a>\n",
    "## 0. Overview\n",
    "\n",
    "We use autocorrect every day on our cell phone and computer. We will explore what really goes on behind the scenes. Of course, the model we are about to implement is not identical to the one used in our phone, but it is still quite good. \n",
    "\n",
    "- Get a word count given a corpus\n",
    "- Get a word probability in the corpus \n",
    "- Manipulate strings \n",
    "- Filter strings \n",
    "- Implement Minimum edit distance to compare strings and to help find the optimal path for the edits. \n",
    "- Understand how dynamic programming works\n",
    "\n",
    "\n",
    "Similar systems are used everywhere. \n",
    "- For example, if you type in the word **\"I am lerningg\"**, chances are very high that you meant to write **\"learning\"**\n",
    "\n",
    "The Turkish translation of Sapiens: A Brief History of Humanind, written by Yuval Noah Harari, has been used as a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_name):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        A file_name which is found in your current directory. You just have to read it in. \n",
    "    Output: \n",
    "        words: a list containing all the words in the corpus (text file you read) in lower case. \n",
    "    \"\"\"\n",
    "    words = [] # return this variable correctly\n",
    "\n",
    "    with open(file_name, encoding='UTF-8') as f:\n",
    "        contents3 = f.read()\n",
    "\n",
    "    contents2 = contents3.lower()\n",
    "    words = re.findall(r'\\w+',contents2)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['hayvanlardan', 'tanrilara', 'sapiens', 'i', 'nsan', 'türünün', 'kısa', 'bir', 'tarihi', 'yuval']\n",
      "There are 24730 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_l = process_data('sapiens.txt')\n",
    "vocab = set(word_l)  # this will be new vocabulary\n",
    "print(f\"The first ten words in the text are: \\n{word_l[0:10]}\")\n",
    "print(f\"There are {len(vocab)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(word_l):\n",
    "    '''\n",
    "    Input:\n",
    "        word_l: a set of words representing the corpus. \n",
    "    Output:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    '''\n",
    "    \n",
    "    word_count_dict = {}  # fill this with word counts\n",
    "    word_count_dict = Counter(word_l)\n",
    "\n",
    "    return word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 24730 key values pairs\n",
      "The count for the word 'thee' is 325\n"
     ]
    }
   ],
   "source": [
    "word_count_dict = get_count(word_l)\n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'thee' is {word_count_dict.get('insan',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(word_count_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        word_count_dict: The wordcount dictionary where key is the word and value is its frequency.\n",
    "    Output:\n",
    "        probs: A dictionary where keys are the words and the values are the probability that a word will occur. \n",
    "    '''\n",
    "    probs = {}  # return this variable correctly\n",
    "\n",
    "    for i in word_count_dict.keys():\n",
    "        probs[i] = word_count_dict[i] / sum(word_count_dict.values())\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 24730\n",
      "P('insan') is 0.0030\n"
     ]
    }
   ],
   "source": [
    "\n",
    "probs = get_probs(word_count_dict)\n",
    "print(f\"Length of probs is {len(probs)}\")\n",
    "print(f\"P('insan') is {probs['insan']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: String Manipulations\n",
    "\n",
    "Now, I have computed $P(w_i)$ for all the words in the corpus, you will write a few functions to manipulate strings so that you can edit the erroneous strings and return the right spellings of the words. In this section, I implemented four functions: \n",
    "\n",
    "* `delete_letter`: given a word, it returns all the possible strings that have **one character removed**. \n",
    "* `switch_letter`: given a word, it returns all the possible strings that have **two adjacent letters switched**.\n",
    "* `replace_letter`: given a word, it returns all the possible strings that have **one character replaced by another different letter**.\n",
    "* `insert_letter`: given a word, it returns all the possible strings that have an **additional character inserted**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def delete_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the string/word for which you will generate all possible words \n",
    "                in the vocabulary which have 1 missing character\n",
    "    Output:\n",
    "        delete_l: a list of all possible strings obtained by deleting 1 character from word\n",
    "    '''\n",
    "    \n",
    "    delete_l = []\n",
    "    split_l = []\n",
    "\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    delete_l = [left+right[1:] for left,right in split_l if right ]\n",
    "\n",
    "    if verbose: print(f\"input word {word}, \\nsplit_l = {split_l}, \\ndelete_l = {delete_l}\")\n",
    "\n",
    "    return delete_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word insan, \n",
      "split_l = [('', 'insan'), ('i', 'nsan'), ('in', 'san'), ('ins', 'an'), ('insa', 'n')], \n",
      "delete_l = ['nsan', 'isan', 'inan', 'insn', 'insa']\n"
     ]
    }
   ],
   "source": [
    "delete_word_l = delete_letter(word=\"insan\",\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def switch_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: input string\n",
    "     Output:\n",
    "        switches: a list of all possible strings with one adjacent charater switched\n",
    "    ''' \n",
    "    \n",
    "    switch_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i],word[i:])for i in range(len(word))]\n",
    "    switch_l =[left[:-1] + right[0] + left[-1] + right[1:]  for left,right in split_l if left]\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nswitch_l = {switch_l}\") \n",
    "\n",
    "    return switch_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = insan \n",
      "split_l = [('', 'insan'), ('i', 'nsan'), ('in', 'san'), ('ins', 'an'), ('insa', 'n')] \n",
      "switch_l = ['nisan', 'isnan', 'inasn', 'insna']\n"
     ]
    }
   ],
   "source": [
    "switch_word_l = switch_letter(word=\"insan\",\n",
    "                         verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        replaces: a list of all possible strings where we replaced one letter from the original word. \n",
    "    ''' \n",
    "    \n",
    "    letters = 'abcçdefgğhıijklmnoöpqrsştuüvyz'\n",
    "    replace_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    split_l = [(word[:i], word[i:]) for i in range(len(word))] \n",
    "    replace_ll = [left + c + right[1:] for left, right in split_l for c in letters if not (left + c + right[1:]==word)]\n",
    "    replace_set = set(replace_ll)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # turn the set back into a list and sort it, for easier viewing\n",
    "    replace_l = sorted(list(replace_set))\n",
    "    \n",
    "    if verbose: print(f\"Input word = {word} \\nsplit_l = {split_l} \\nreplace_l {replace_l}\")   \n",
    "    \n",
    "    return replace_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word = insan \n",
      "split_l = [('', 'insan'), ('i', 'nsan'), ('in', 'san'), ('ins', 'an'), ('insa', 'n')] \n",
      "replace_l ['ansan', 'bnsan', 'cnsan', 'dnsan', 'ensan', 'fnsan', 'gnsan', 'hnsan', 'iasan', 'ibsan', 'icsan', 'idsan', 'iesan', 'ifsan', 'igsan', 'ihsan', 'iisan', 'ijsan', 'iksan', 'ilsan', 'imsan', 'inaan', 'inban', 'incan', 'indan', 'inean', 'infan', 'ingan', 'inhan', 'inian', 'injan', 'inkan', 'inlan', 'inman', 'innan', 'inoan', 'inpan', 'inqan', 'inran', 'insaa', 'insab', 'insac', 'insad', 'insae', 'insaf', 'insag', 'insah', 'insai', 'insaj', 'insak', 'insal', 'insam', 'insao', 'insap', 'insaq', 'insar', 'insas', 'insat', 'insau', 'insav', 'insay', 'insaz', 'insaç', 'insaö', 'insaü', 'insağ', 'insaı', 'insaş', 'insbn', 'inscn', 'insdn', 'insen', 'insfn', 'insgn', 'inshn', 'insin', 'insjn', 'inskn', 'insln', 'insmn', 'insnn', 'inson', 'inspn', 'insqn', 'insrn', 'inssn', 'instn', 'insun', 'insvn', 'insyn', 'inszn', 'insçn', 'insön', 'insün', 'insğn', 'insın', 'insşn', 'intan', 'inuan', 'invan', 'inyan', 'inzan', 'inçan', 'inöan', 'inüan', 'inğan', 'inıan', 'inşan', 'iosan', 'ipsan', 'iqsan', 'irsan', 'issan', 'itsan', 'iusan', 'ivsan', 'iysan', 'izsan', 'içsan', 'iösan', 'iüsan', 'iğsan', 'iısan', 'işsan', 'jnsan', 'knsan', 'lnsan', 'mnsan', 'nnsan', 'onsan', 'pnsan', 'qnsan', 'rnsan', 'snsan', 'tnsan', 'unsan', 'vnsan', 'ynsan', 'znsan', 'çnsan', 'önsan', 'ünsan', 'ğnsan', 'ınsan', 'şnsan']\n"
     ]
    }
   ],
   "source": [
    "replace_l = replace_letter(word='insan',\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_letter(word, verbose=False):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        inserts: a set of all possible strings with one new letter inserted at every offset\n",
    "    ''' \n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    insert_l = []\n",
    "    split_l = []\n",
    "    \n",
    "    split_l = [(word[:i],word[i:]) for i in range(len(word) + 1)]\n",
    "    insert_l = [left + c + right for left,right in split_l for c in letters]\n",
    "   \n",
    "    if verbose: print(f\"Input word {word} \\nsplit_l = {split_l} \\ninsert_l = {insert_l}\")\n",
    "    \n",
    "    return insert_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input word insan \n",
      "split_l = [('', 'insan'), ('i', 'nsan'), ('in', 'san'), ('ins', 'an'), ('insa', 'n'), ('insan', '')] \n",
      "insert_l = ['ainsan', 'binsan', 'cinsan', 'dinsan', 'einsan', 'finsan', 'ginsan', 'hinsan', 'iinsan', 'jinsan', 'kinsan', 'linsan', 'minsan', 'ninsan', 'oinsan', 'pinsan', 'qinsan', 'rinsan', 'sinsan', 'tinsan', 'uinsan', 'vinsan', 'winsan', 'xinsan', 'yinsan', 'zinsan', 'iansan', 'ibnsan', 'icnsan', 'idnsan', 'iensan', 'ifnsan', 'ignsan', 'ihnsan', 'iinsan', 'ijnsan', 'iknsan', 'ilnsan', 'imnsan', 'innsan', 'ionsan', 'ipnsan', 'iqnsan', 'irnsan', 'isnsan', 'itnsan', 'iunsan', 'ivnsan', 'iwnsan', 'ixnsan', 'iynsan', 'iznsan', 'inasan', 'inbsan', 'incsan', 'indsan', 'inesan', 'infsan', 'ingsan', 'inhsan', 'inisan', 'injsan', 'inksan', 'inlsan', 'inmsan', 'innsan', 'inosan', 'inpsan', 'inqsan', 'inrsan', 'inssan', 'intsan', 'inusan', 'invsan', 'inwsan', 'inxsan', 'inysan', 'inzsan', 'insaan', 'insban', 'inscan', 'insdan', 'insean', 'insfan', 'insgan', 'inshan', 'insian', 'insjan', 'inskan', 'inslan', 'insman', 'insnan', 'insoan', 'inspan', 'insqan', 'insran', 'inssan', 'instan', 'insuan', 'insvan', 'inswan', 'insxan', 'insyan', 'inszan', 'insaan', 'insabn', 'insacn', 'insadn', 'insaen', 'insafn', 'insagn', 'insahn', 'insain', 'insajn', 'insakn', 'insaln', 'insamn', 'insann', 'insaon', 'insapn', 'insaqn', 'insarn', 'insasn', 'insatn', 'insaun', 'insavn', 'insawn', 'insaxn', 'insayn', 'insazn', 'insana', 'insanb', 'insanc', 'insand', 'insane', 'insanf', 'insang', 'insanh', 'insani', 'insanj', 'insank', 'insanl', 'insanm', 'insann', 'insano', 'insanp', 'insanq', 'insanr', 'insans', 'insant', 'insanu', 'insanv', 'insanw', 'insanx', 'insany', 'insanz']\n",
      "Number of strings output by insert_letter('at') is 156\n"
     ]
    }
   ],
   "source": [
    "insert_l = insert_letter('insan', True)\n",
    "print(f\"Number of strings output by insert_letter('at') is {len(insert_l)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edit_one_letter(word, allow_switches = True):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        word: the string/word for which we will generate all possible wordsthat are one edit away.\n",
    "    Output:\n",
    "        edit_one_set: a set of words with one possible edit. Please return a set. and not a list.\n",
    "    \"\"\"\n",
    "    \n",
    "    edit_one_set = set()\n",
    "    one_list = insert_letter(word) + delete_letter(word) + replace_letter(word)\n",
    "    if allow_switches:\n",
    "        edit_one_set = set(switch_letter(word) + one_list)\n",
    "    else:\n",
    "        edit_one_set = set(one_list)\n",
    "\n",
    "    return edit_one_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input word insan \n",
      "edit_one_l \n",
      "['ainsan', 'ansan', 'binsan', 'bnsan', 'cinsan', 'cnsan', 'dinsan', 'dnsan', 'einsan', 'ensan', 'finsan', 'fnsan', 'ginsan', 'gnsan', 'hinsan', 'hnsan', 'iansan', 'iasan', 'ibnsan', 'ibsan', 'icnsan', 'icsan', 'idnsan', 'idsan', 'iensan', 'iesan', 'ifnsan', 'ifsan', 'ignsan', 'igsan', 'ihnsan', 'ihsan', 'iinsan', 'iisan', 'ijnsan', 'ijsan', 'iknsan', 'iksan', 'ilnsan', 'ilsan', 'imnsan', 'imsan', 'inaan', 'inan', 'inasan', 'inasn', 'inban', 'inbsan', 'incan', 'incsan', 'indan', 'indsan', 'inean', 'inesan', 'infan', 'infsan', 'ingan', 'ingsan', 'inhan', 'inhsan', 'inian', 'inisan', 'injan', 'injsan', 'inkan', 'inksan', 'inlan', 'inlsan', 'inman', 'inmsan', 'innan', 'innsan', 'inoan', 'inosan', 'inpan', 'inpsan', 'inqan', 'inqsan', 'inran', 'inrsan', 'insa', 'insaa', 'insaan', 'insab', 'insabn', 'insac', 'insacn', 'insad', 'insadn', 'insae', 'insaen', 'insaf', 'insafn', 'insag', 'insagn', 'insah', 'insahn', 'insai', 'insain', 'insaj', 'insajn', 'insak', 'insakn', 'insal', 'insaln', 'insam', 'insamn', 'insana', 'insanb', 'insanc', 'insand', 'insane', 'insanf', 'insang', 'insanh', 'insani', 'insanj', 'insank', 'insanl', 'insanm', 'insann', 'insano', 'insanp', 'insanq', 'insanr', 'insans', 'insant', 'insanu', 'insanv', 'insanw', 'insanx', 'insany', 'insanz', 'insao', 'insaon', 'insap', 'insapn', 'insaq', 'insaqn', 'insar', 'insarn', 'insas', 'insasn', 'insat', 'insatn', 'insau', 'insaun', 'insav', 'insavn', 'insawn', 'insaxn', 'insay', 'insayn', 'insaz', 'insazn', 'insaç', 'insaö', 'insaü', 'insağ', 'insaı', 'insaş', 'insban', 'insbn', 'inscan', 'inscn', 'insdan', 'insdn', 'insean', 'insen', 'insfan', 'insfn', 'insgan', 'insgn', 'inshan', 'inshn', 'insian', 'insin', 'insjan', 'insjn', 'inskan', 'inskn', 'inslan', 'insln', 'insman', 'insmn', 'insn', 'insna', 'insnan', 'insnn', 'insoan', 'inson', 'inspan', 'inspn', 'insqan', 'insqn', 'insran', 'insrn', 'inssan', 'inssn', 'instan', 'instn', 'insuan', 'insun', 'insvan', 'insvn', 'inswan', 'insxan', 'insyan', 'insyn', 'inszan', 'inszn', 'insçn', 'insön', 'insün', 'insğn', 'insın', 'insşn', 'intan', 'intsan', 'inuan', 'inusan', 'invan', 'invsan', 'inwsan', 'inxsan', 'inyan', 'inysan', 'inzan', 'inzsan', 'inçan', 'inöan', 'inüan', 'inğan', 'inıan', 'inşan', 'ionsan', 'iosan', 'ipnsan', 'ipsan', 'iqnsan', 'iqsan', 'irnsan', 'irsan', 'isan', 'isnan', 'isnsan', 'issan', 'itnsan', 'itsan', 'iunsan', 'iusan', 'ivnsan', 'ivsan', 'iwnsan', 'ixnsan', 'iynsan', 'iysan', 'iznsan', 'izsan', 'içsan', 'iösan', 'iüsan', 'iğsan', 'iısan', 'işsan', 'jinsan', 'jnsan', 'kinsan', 'knsan', 'linsan', 'lnsan', 'minsan', 'mnsan', 'ninsan', 'nisan', 'nnsan', 'nsan', 'oinsan', 'onsan', 'pinsan', 'pnsan', 'qinsan', 'qnsan', 'rinsan', 'rnsan', 'sinsan', 'snsan', 'tinsan', 'tnsan', 'uinsan', 'unsan', 'vinsan', 'vnsan', 'winsan', 'xinsan', 'yinsan', 'ynsan', 'zinsan', 'znsan', 'çnsan', 'önsan', 'ünsan', 'ğnsan', 'ınsan', 'şnsan']\n",
      "\n",
      "The type of the returned object should be a set <class 'set'>\n",
      "Number of outputs from edit_one_letter('at') is 137\n"
     ]
    }
   ],
   "source": [
    "tmp_word = \"insan\"\n",
    "tmp_edit_one_set = edit_one_letter(tmp_word)\n",
    "# turn this into a list to sort it, in order to view it\n",
    "tmp_edit_one_l = sorted(list(tmp_edit_one_set))\n",
    "\n",
    "print(f\"input word {tmp_word} \\nedit_one_l \\n{tmp_edit_one_l}\\n\")\n",
    "print(f\"The type of the returned object should be a set {type(tmp_edit_one_set)}\")\n",
    "print(f\"Number of outputs from edit_one_letter('at') is {len(edit_one_letter('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def edit_two_letters(word, allow_switches = True):\n",
    "    '''\n",
    "    Input:\n",
    "        word: the input string/word \n",
    "    Output:\n",
    "        edit_two_set: a set of strings with all possible two edits\n",
    "    '''    \n",
    "    edit_two_set = set()\n",
    "\n",
    "    for new_word in edit_one_letter(word):\n",
    "        edit_two_set = edit_two_set|edit_one_letter(new_word, allow_switches=allow_switches)\n",
    "   \n",
    "    return edit_two_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of strings with edit distance of two: 42709\n",
      "First 10 strings ['aainsan', 'aansan', 'aasan', 'abinsan', 'abnsan', 'absan', 'acinsan', 'acnsan', 'acsan', 'adinsan']\n",
      "Last 10 strings ['şynsan', 'şysan', 'şznsan', 'şzsan', 'şçsan', 'şösan', 'şüsan', 'şğsan', 'şısan', 'şşsan']\n",
      "The data type of the returned object should be a set <class 'set'>\n",
      "Number of strings that are 2 edit distances from 'at' is 8204\n"
     ]
    }
   ],
   "source": [
    "tmp_edit_two_set = edit_two_letters(\"insan\")\n",
    "tmp_edit_two_l = sorted(list(tmp_edit_two_set))\n",
    "print(f\"Number of strings with edit distance of two: {len(tmp_edit_two_l)}\")\n",
    "print(f\"First 10 strings {tmp_edit_two_l[:10]}\")\n",
    "print(f\"Last 10 strings {tmp_edit_two_l[-10:]}\")\n",
    "print(f\"The data type of the returned object should be a set {type(tmp_edit_two_set)}\")\n",
    "print(f\"Number of strings that are 2 edit distances from 'at' is {len(edit_two_letters('at'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corrections(word, probs, vocab, n=2, verbose = False):\n",
    "    '''\n",
    "    Input: \n",
    "        word: a user entered string to check for suggestions\n",
    "        probs: a dictionary that maps each word to its probability in the corpus\n",
    "        vocab: a set containing all the vocabulary\n",
    "        n: number of possible word corrections you want returned in the dictionary\n",
    "    Output: \n",
    "        n_best: a list of tuples with the most probable n corrected words and their probabilities.\n",
    "    '''\n",
    "    \n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "\n",
    "    if word not in vocab:\n",
    "        edit_one = edit_one_letter(word)\n",
    "        edit_one_vocab =edit_one.intersection(vocab)\n",
    "        edit_two = edit_two_letters(word)\n",
    "        edit_two_vocab = edit_two.intersection(vocab)\n",
    "        suggestions = list(edit_one_vocab) or list(edit_two_vocab)\n",
    "        prob_suggestions = [probs[i] for i in suggestions]\n",
    "        n_best_inx = np.argsort(prob_suggestions)[-3:][::-1]#n olacak\n",
    "        n_best = [(suggestions[i],prob_suggestions[i]) for i in n_best_inx]\n",
    "    else:\n",
    "        n_best = [(word,1)]\n",
    "\n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word =  insson \n",
      "suggestions =  ['insan']\n",
      "word 0: insan, probability 0.003040\n",
      "data type of corrections <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Testing implementation - feel free to try other words in my word\n",
    "my_word = 'insson' \n",
    "tmp_corrections = get_corrections(my_word, probs, vocab, 2, verbose=True) # keep verbose=True\n",
    "for i, word_prob in enumerate(tmp_corrections):\n",
    "    print(f\"word {i}: {word_prob[0]}, probability {word_prob[1]:.6f}\")\n",
    "\n",
    "print(f\"data type of corrections {type(tmp_corrections)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Minimum Edit Distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def min_edit_distance(source, target, ins_cost = 1, del_cost = 1, rep_cost = 2):\n",
    "    '''\n",
    "    Input: \n",
    "        source: a string corresponding to the string you are starting with\n",
    "        target: a string corresponding to the string you want to end with\n",
    "        ins_cost: an integer setting the insert cost\n",
    "        del_cost: an integer setting the delete cost\n",
    "        rep_cost: an integer setting the replace cost\n",
    "    Output:\n",
    "        D: a matrix of len(source)+1 by len(target)+1 containing minimum edit distances\n",
    "        med: the minimum edit distance (med) required to convert the source string to the target\n",
    "    '''\n",
    "   \n",
    "    m = len(source) \n",
    "    n = len(target) \n",
    "    \n",
    "    D = np.zeros((m+1, n+1), dtype=int)    \n",
    "\n",
    "    for row in range(1,m+1): \n",
    "        D[row,0] = D[row-1,0] + 1\n",
    "        \n",
    "    for col in range(1,n+1): \n",
    "        D[0,col] = D[0,col-1] + 1\n",
    "        \n",
    "    for row in range(1,m+1): \n",
    "        \n",
    "        for col in range(1,n+1):\n",
    "            \n",
    "            r_cost = rep_cost          \n",
    "                \n",
    "            if source[row-1] == target[col-1]:\n",
    "               \n",
    "                r_cost = 0                \n",
    "            \n",
    "            D[row,col] = min(D[row-1,col]+del_cost,D[row,col-1]+ins_cost, D[row-1,col-1]+r_cost)\n",
    "    \n",
    "    med = D[m,n]\n",
    "    \n",
    "    return D, med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum edits:  4 \n",
      "\n",
      "   #  i  n  s  a  n   \n",
      "#  0  1  2  3  4  5  6\n",
      "i  1  0  1  2  3  4  5\n",
      "n  2  1  0  1  2  3  4\n",
      "s  3  2  1  0  1  2  3\n",
      "s  4  3  2  1  2  3  4\n",
      "o  5  4  3  2  3  4  5\n",
      "n  6  5  4  3  4  3  4\n"
     ]
    }
   ],
   "source": [
    "source =  'insson'\n",
    "target = 'insan '\n",
    "matrix, min_edits = min_edit_distance(source, target)\n",
    "print(\"minimum edits: \",min_edits, \"\\n\")\n",
    "idx = list('#' + source)\n",
    "cols = list('#' + target)\n",
    "df = pd.DataFrame(matrix, index=idx, columns= cols)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum edit distance is lower than 2 among one letter edit selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inson insan 1\n"
     ]
    }
   ],
   "source": [
    "source = \"inson\"\n",
    "targets = edit_one_letter(source,allow_switches = False)  #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits <2 and t in vocab: print(source, t, min_edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum edit distance is lower than 4 among two letter edit selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inson insek 2\n",
      "inson insan 1\n",
      "inson insana 2\n",
      "inson son 2\n",
      "inson insani 2\n",
      "inson inin 2\n",
      "inson anton 2\n",
      "inson nsan 2\n",
      "inson nisan 3\n"
     ]
    }
   ],
   "source": [
    "source = \"inson\"\n",
    "targets = edit_two_letters(source,allow_switches = False) #disable switches since min_edit_distance does not include them\n",
    "for t in targets:\n",
    "    _, min_edits = min_edit_distance(source, t,1,1,1)  # set ins, del, sub costs all to one\n",
    "    if min_edits <4 and t in vocab: print(source, t, min_edits)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC2-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
